
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU &#8212; Draugr 1.0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <link rel="canonical" href="pything.github.io/draugr/generated/draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="draugr.torch_utilities.optimisation.debugging.layer_fetching" href="draugr.torch_utilities.optimisation.debugging.layer_fetching.html" />
    <link rel="prev" title="draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLUModel" href="draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLUModel.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="draugr-torch-utilities-optimisation-debugging-gradients-guided-guidedbackproprelu">
<h1>draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU<a class="headerlink" href="#draugr-torch-utilities-optimisation-debugging-gradients-guided-guidedbackproprelu" title="Permalink to this heading">¶</a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">draugr.torch_utilities.optimisation.debugging.gradients.guided.</span></span><span class="sig-name descname"><span class="pre">GuidedBackPropReLU</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/draugr/torch_utilities/optimisation/debugging/gradients/guided.html#GuidedBackPropReLU"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Function</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.__init__" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p class="rubric">Methods</p>
<table class="autosummary longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.__init__" title="draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.__init__"><code class="xref py py-obj docutils literal notranslate"><span class="pre">__init__</span></code></a>(*args, **kwargs)</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">apply</span></code></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.backward" title="draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.backward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">backward</span></code></a>(self, grad_output)</p></td>
<td><p><dl class="field-list simple">
<dt class="field-odd">param self</dt>
<dd class="field-odd"><p></p></dd>
</dl>
</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.forward" title="draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.forward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">forward</span></code></a>(self, input_img)</p></td>
<td><p><dl class="field-list simple">
<dt class="field-odd">param self</dt>
<dd class="field-odd"><p></p></dd>
</dl>
</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.jvp" title="draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.jvp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">jvp</span></code></a>(ctx, *grad_inputs)</p></td>
<td><p>Defines a formula for differentiating the operation with forward mode automatic differentiation.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.mark_dirty" title="draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.mark_dirty"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mark_dirty</span></code></a>(*args)</p></td>
<td><p>Marks given tensors as modified in an in-place operation.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.mark_non_differentiable" title="draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.mark_non_differentiable"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mark_non_differentiable</span></code></a>(*args)</p></td>
<td><p>Marks outputs as non-differentiable.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">mark_shared_storage</span></code>(*pairs)</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">name</span></code></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_hook</span></code></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.save_for_backward" title="draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.save_for_backward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">save_for_backward</span></code></a>(*tensors)</p></td>
<td><p>Saves given tensors for a future call to <code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.save_for_forward" title="draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.save_for_forward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">save_for_forward</span></code></a>(*tensors)</p></td>
<td><p>Saves given tensors for a future call to <code class="xref py py-func docutils literal notranslate"><span class="pre">jvp()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.set_materialize_grads" title="draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.set_materialize_grads"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_materialize_grads</span></code></a>(value)</p></td>
<td><p>Sets whether to materialize output grad tensors.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.vjp" title="draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.vjp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">vjp</span></code></a>(ctx, *grad_outputs)</p></td>
<td><p>Defines a formula for differentiating the operation with backward mode automatic differentiation (alias to the vjp function).</p></td>
</tr>
</tbody>
</table>
<p class="rubric">Attributes</p>
<table class="autosummary longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">dirty_tensors</span></code></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_traceable</span></code></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">materialize_grads</span></code></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">metadata</span></code></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">needs_input_grad</span></code></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">next_functions</span></code></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">non_differentiable</span></code></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">requires_grad</span></code></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">saved_for_forward</span></code></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">saved_tensors</span></code></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">saved_variables</span></code></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">to_save</span></code></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt class="sig sig-object py" id="draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.backward">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_output</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/draugr/torch_utilities/optimisation/debugging/gradients/guided.html#GuidedBackPropReLU.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.backward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self</strong> – </p></li>
<li><p><strong>grad_output</strong> – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.forward">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_img</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/draugr/torch_utilities/optimisation/debugging/gradients/guided.html#GuidedBackPropReLU.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self</strong> – </p></li>
<li><p><strong>input_img</strong> – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.jvp">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">jvp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.10)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">grad_inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.10)"><span class="pre">Any</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.10)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.jvp" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines a formula for differentiating the operation with forward mode
automatic differentiation.
This function is to be overridden by all subclasses.
It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many inputs as the <a class="reference internal" href="#draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.forward" title="draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> got (None will be passed in
for non tensor inputs of the forward function),
and it should return as many tensors as there were outputs to
<a class="reference internal" href="#draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.forward" title="draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a>. Each argument is the gradient w.r.t the given input,
and each returned value should be the gradient w.r.t. the
corresponding output. If an output is not a Tensor or the function is not
differentiable with respect to that output, you can just pass None as a
gradient for that input.</p>
<p>You can use the <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> object to pass any value from the forward to this
functions.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.mark_dirty">
<span class="sig-name descname"><span class="pre">mark_dirty</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.mark_dirty" title="Permalink to this definition">¶</a></dt>
<dd><p>Marks given tensors as modified in an in-place operation.</p>
<p><strong>This should be called at most once, only from inside the</strong>
<a class="reference internal" href="#draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.forward" title="draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> <strong>method, and all arguments should be inputs.</strong></p>
<p>Every tensor that’s been modified in-place in a call to <a class="reference internal" href="#draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.forward" title="draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a>
should be given to this function, to ensure correctness of our checks.
It doesn’t matter whether the function is called before or after
modification.</p>
<dl>
<dt>Examples::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Inplace</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x_npy</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="c1"># x_npy shares storage with x</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x_npy</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">mark_dirty</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">grad_output</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">a</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Inplace</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>  <span class="c1"># This would lead to wrong gradients!</span>
<span class="gp">&gt;&gt;&gt; </span>                  <span class="c1"># but the engine would not know unless we mark_dirty</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># RuntimeError: one of the variables needed for gradient</span>
<span class="gp">&gt;&gt;&gt; </span>             <span class="c1"># computation has been modified by an inplace operation</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.mark_non_differentiable">
<span class="sig-name descname"><span class="pre">mark_non_differentiable</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.mark_non_differentiable" title="Permalink to this definition">¶</a></dt>
<dd><p>Marks outputs as non-differentiable.</p>
<p><strong>This should be called at most once, only from inside the</strong>
<a class="reference internal" href="#draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.forward" title="draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> <strong>method, and all arguments should be tensor outputs.</strong></p>
<p>This will mark outputs as not requiring gradients, increasing the
efficiency of backward computation. You still need to accept a gradient
for each output in <code class="xref py py-meth docutils literal notranslate"><span class="pre">backward()</span></code>, but it’s always going to
be a zero tensor with the same shape as the shape of a corresponding
output.</p>
<dl>
<dt>This is used e.g. for indices returned from a sort. See example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Func</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">sorted</span><span class="p">,</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">mark_non_differentiable</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="nb">sorted</span><span class="p">,</span> <span class="n">idx</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">g1</span><span class="p">,</span> <span class="n">g2</span><span class="p">):</span>  <span class="c1"># still need to accept g2</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">grad_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">grad_input</span><span class="o">.</span><span class="n">index_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">g1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">grad_input</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.save_for_backward">
<span class="sig-name descname"><span class="pre">save_for_backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">tensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.save_for_backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves given tensors for a future call to <code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">save_for_backward</span></code> should be called at most once, only from inside the
<a class="reference internal" href="#draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.forward" title="draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> method, and only with tensors.</p>
<p>All tensors intended to be used in the backward pass should be saved
with <code class="docutils literal notranslate"><span class="pre">save_for_backward</span></code> (as opposed to directly on <code class="docutils literal notranslate"><span class="pre">ctx</span></code>) to prevent
incorrect gradients and memory leaks, and enable the application of saved
tensor hooks. See <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.graph.saved_tensors_hooks</span></code>.</p>
<p>In <a class="reference internal" href="#draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.backward" title="draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a>, saved tensors can be accessed through the <code class="xref py py-attr docutils literal notranslate"><span class="pre">saved_tensors</span></code>
attribute. Before returning them to the user, a check is made to ensure
they weren’t used in any in-place operation that modified their content.</p>
<p>Arguments can also be <code class="docutils literal notranslate"><span class="pre">None</span></code>. This is a no-op.</p>
<p>See <span class="xref std std-ref">extending-autograd</span> for more details on how to use this method.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Func</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">w</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span> <span class="o">*</span> <span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span> <span class="o">+</span> <span class="n">y</span> <span class="o">*</span> <span class="n">z</span> <span class="o">+</span> <span class="n">w</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="n">z</span>  <span class="c1"># z is not a tensor</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">out</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_out</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">out</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">z</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">gx</span> <span class="o">=</span> <span class="n">grad_out</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="n">y</span> <span class="o">*</span> <span class="n">z</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">gy</span> <span class="o">=</span> <span class="n">grad_out</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">z</span> <span class="o">+</span> <span class="n">x</span> <span class="o">*</span> <span class="n">z</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">gz</span> <span class="o">=</span> <span class="kc">None</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">gx</span><span class="p">,</span> <span class="n">gy</span><span class="p">,</span> <span class="n">gz</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">Func</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.save_for_forward">
<span class="sig-name descname"><span class="pre">save_for_forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">tensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.save_for_forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves given tensors for a future call to <code class="xref py py-func docutils literal notranslate"><span class="pre">jvp()</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">save_for_forward</span></code> should be only called once, from inside the <a class="reference internal" href="#draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.forward" title="draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a>
method, and only be called with tensors.</p>
<p>In <a class="reference internal" href="#draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.jvp" title="draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.jvp"><code class="xref py py-func docutils literal notranslate"><span class="pre">jvp()</span></code></a>, saved objects can be accessed through the <code class="xref py py-attr docutils literal notranslate"><span class="pre">saved_tensors</span></code>
attribute.</p>
<p>Arguments can also be <code class="docutils literal notranslate"><span class="pre">None</span></code>. This is a no-op.</p>
<p>See <span class="xref std std-ref">extending-autograd</span> for more details on how to use this method.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Func</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span> <span class="o">*</span> <span class="n">z</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">jvp</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x_t</span><span class="p">,</span> <span class="n">y_t</span><span class="p">,</span> <span class="n">_</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">z</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">z</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="n">x_t</span> <span class="o">+</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y_t</span><span class="p">)</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">vjp</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_out</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">z</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">z</span> <span class="o">*</span> <span class="n">grad_out</span> <span class="o">*</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span> <span class="o">*</span> <span class="n">grad_out</span> <span class="o">*</span> <span class="n">x</span><span class="p">,</span> <span class="kc">None</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">c</span> <span class="o">=</span> <span class="mi">4</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">with</span> <span class="n">fwAD</span><span class="o">.</span><span class="n">dual_level</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">a_dual</span> <span class="o">=</span> <span class="n">fwAD</span><span class="o">.</span><span class="n">make_dual</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">d</span> <span class="o">=</span> <span class="n">Func</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a_dual</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.set_materialize_grads">
<span class="sig-name descname"><span class="pre">set_materialize_grads</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><span class="pre">bool</span></a></span></em><span class="sig-paren">)</span><a class="headerlink" href="#draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.set_materialize_grads" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets whether to materialize output grad tensors. Default is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<p><strong>This should be called only from inside the</strong> <a class="reference internal" href="#draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.forward" title="draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> <strong>method</strong></p>
<p>If <code class="docutils literal notranslate"><span class="pre">True</span></code>, undefined output grad tensors will be expanded to tensors full
of zeros prior to calling the <a class="reference internal" href="#draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.backward" title="draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a> method.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">SimpleFunc</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">g1</span><span class="p">,</span> <span class="n">g2</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">g1</span> <span class="o">+</span> <span class="n">g2</span>  <span class="c1"># No check for None necessary</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># We modify SimpleFunc to handle non-materialized grad outputs</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Func</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">set_materialize_grads</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">g1</span><span class="p">,</span> <span class="n">g2</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">grad_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">if</span> <span class="n">g1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># We must check for None now</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="n">grad_input</span> <span class="o">+=</span> <span class="n">g1</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">if</span> <span class="n">g2</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="n">grad_input</span> <span class="o">+=</span> <span class="n">g2</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">grad_input</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">Func</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>  <span class="c1"># induces g2 to be undefined</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.vjp">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">vjp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.10)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">grad_outputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.10)"><span class="pre">Any</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.10)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.vjp" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines a formula for differentiating the operation with backward mode
automatic differentiation (alias to the vjp function).</p>
<p>This function is to be overridden by all subclasses.</p>
<p>It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many outputs as the <a class="reference internal" href="#draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.forward" title="draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> returned (None will be passed in
for non tensor outputs of the forward function),
and it should return as many tensors, as there were inputs to
<a class="reference internal" href="#draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.forward" title="draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a>. Each argument is the gradient w.r.t the given output,
and each returned value should be the gradient w.r.t. the
corresponding input. If an input is not a Tensor or is a Tensor not
requiring grads, you can just pass None as a gradient for that input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx.needs_input_grad</span></code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
<a class="reference internal" href="#draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.backward" title="draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a> will have <code class="docutils literal notranslate"><span class="pre">ctx.needs_input_grad[0]</span> <span class="pre">=</span> <span class="pre">True</span></code> if the
first input to <a class="reference internal" href="#draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.forward" title="draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> needs gradient computated w.r.t. the
output.</p>
</dd></dl>

</dd></dl>

</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../index.html">
              <img class="logo" src="../_static/valknut.svg" alt="Logo"/>
            </a></p>
<h1 class="logo"><a href="../index.html">Draugr</a></h1>








<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="draugr.html">draugr</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="draugr.dist_is_editable.html">draugr.dist_is_editable</a></li>
<li class="toctree-l2"><a class="reference internal" href="draugr.get_version.html">draugr.get_version</a></li>
<li class="toctree-l2"><a class="reference internal" href="draugr.dlib_utilities.html">draugr.dlib_utilities</a></li>
<li class="toctree-l2"><a class="reference internal" href="draugr.drawers.html">draugr.drawers</a></li>
<li class="toctree-l2"><a class="reference internal" href="draugr.entry_points.html">draugr.entry_points</a></li>
<li class="toctree-l2"><a class="reference internal" href="draugr.extensions.html">draugr.extensions</a></li>
<li class="toctree-l2"><a class="reference internal" href="draugr.ffmpeg_utilities.html">draugr.ffmpeg_utilities</a></li>
<li class="toctree-l2"><a class="reference internal" href="draugr.jax_utilities.html">draugr.jax_utilities</a></li>
<li class="toctree-l2"><a class="reference internal" href="draugr.metrics.html">draugr.metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="draugr.multiprocessing_utilities.html">draugr.multiprocessing_utilities</a></li>
<li class="toctree-l2"><a class="reference internal" href="draugr.numpy_utilities.html">draugr.numpy_utilities</a></li>
<li class="toctree-l2"><a class="reference internal" href="draugr.opencv_utilities.html">draugr.opencv_utilities</a></li>
<li class="toctree-l2"><a class="reference internal" href="draugr.os_utilities.html">draugr.os_utilities</a></li>
<li class="toctree-l2"><a class="reference internal" href="draugr.pandas_utilities.html">draugr.pandas_utilities</a></li>
<li class="toctree-l2"><a class="reference internal" href="draugr.pygame_utilities.html">draugr.pygame_utilities</a></li>
<li class="toctree-l2"><a class="reference internal" href="draugr.python_utilities.html">draugr.python_utilities</a></li>
<li class="toctree-l2"><a class="reference internal" href="draugr.random_utilities.html">draugr.random_utilities</a></li>
<li class="toctree-l2"><a class="reference internal" href="draugr.scipy_utilities.html">draugr.scipy_utilities</a></li>
<li class="toctree-l2"><a class="reference internal" href="draugr.stopping.html">draugr.stopping</a></li>
<li class="toctree-l2"><a class="reference internal" href="draugr.tensorboard_utilities.html">draugr.tensorboard_utilities</a></li>
<li class="toctree-l2"><a class="reference internal" href="draugr.threading_utilities.html">draugr.threading_utilities</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="draugr.torch_utilities.html">draugr.torch_utilities</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="draugr.torch_utilities.architectures.html">draugr.torch_utilities.architectures</a></li>
<li class="toctree-l3"><a class="reference internal" href="draugr.torch_utilities.datasets.html">draugr.torch_utilities.datasets</a></li>
<li class="toctree-l3"><a class="reference internal" href="draugr.torch_utilities.distributions.html">draugr.torch_utilities.distributions</a></li>
<li class="toctree-l3"><a class="reference internal" href="draugr.torch_utilities.evaluation.html">draugr.torch_utilities.evaluation</a></li>
<li class="toctree-l3"><a class="reference internal" href="draugr.torch_utilities.exporting.html">draugr.torch_utilities.exporting</a></li>
<li class="toctree-l3"><a class="reference internal" href="draugr.torch_utilities.generators.html">draugr.torch_utilities.generators</a></li>
<li class="toctree-l3"><a class="reference internal" href="draugr.torch_utilities.images.html">draugr.torch_utilities.images</a></li>
<li class="toctree-l3"><a class="reference internal" href="draugr.torch_utilities.opencv.html">draugr.torch_utilities.opencv</a></li>
<li class="toctree-l3"><a class="reference internal" href="draugr.torch_utilities.operations.html">draugr.torch_utilities.operations</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="draugr.torch_utilities.optimisation.html">draugr.torch_utilities.optimisation</a><ul class="current">
<li class="toctree-l4 current"><a class="reference internal" href="draugr.torch_utilities.optimisation.debugging.html">draugr.torch_utilities.optimisation.debugging</a><ul class="current">
<li class="toctree-l5 current"><a class="reference internal" href="draugr.torch_utilities.optimisation.debugging.gradients.html">draugr.torch_utilities.optimisation.debugging.gradients</a><ul class="current">
<li class="toctree-l6"><a class="reference internal" href="draugr.torch_utilities.optimisation.debugging.gradients.checking.html">draugr.torch_utilities.optimisation.debugging.gradients.checking</a></li>
<li class="toctree-l6"><a class="reference internal" href="draugr.torch_utilities.optimisation.debugging.gradients.flow.html">draugr.torch_utilities.optimisation.debugging.gradients.flow</a></li>
<li class="toctree-l6"><a class="reference internal" href="draugr.torch_utilities.optimisation.debugging.gradients.grad_trace.html">draugr.torch_utilities.optimisation.debugging.gradients.grad_trace</a></li>
<li class="toctree-l6 current"><a class="reference internal" href="draugr.torch_utilities.optimisation.debugging.gradients.guided.html">draugr.torch_utilities.optimisation.debugging.gradients.guided</a><ul class="current">
<li class="toctree-l7"><a class="reference internal" href="draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLUModel.html">draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLUModel</a></li>
<li class="toctree-l7 current"><a class="current reference internal" href="#">draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l5"><a class="reference internal" href="draugr.torch_utilities.optimisation.debugging.layer_fetching.html">draugr.torch_utilities.optimisation.debugging.layer_fetching</a></li>
<li class="toctree-l5"><a class="reference internal" href="draugr.torch_utilities.optimisation.debugging.opt_verification.html">draugr.torch_utilities.optimisation.debugging.opt_verification</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="draugr.torch_utilities.optimisation.parameters.html">draugr.torch_utilities.optimisation.parameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="draugr.torch_utilities.optimisation.scheduling.html">draugr.torch_utilities.optimisation.scheduling</a></li>
<li class="toctree-l4"><a class="reference internal" href="draugr.torch_utilities.optimisation.stopping.html">draugr.torch_utilities.optimisation.stopping</a></li>
<li class="toctree-l4"><a class="reference internal" href="draugr.torch_utilities.optimisation.updates.html">draugr.torch_utilities.optimisation.updates</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="draugr.torch_utilities.persistence.html">draugr.torch_utilities.persistence</a></li>
<li class="toctree-l3"><a class="reference internal" href="draugr.torch_utilities.sessions.html">draugr.torch_utilities.sessions</a></li>
<li class="toctree-l3"><a class="reference internal" href="draugr.torch_utilities.system.html">draugr.torch_utilities.system</a></li>
<li class="toctree-l3"><a class="reference internal" href="draugr.torch_utilities.tensors.html">draugr.torch_utilities.tensors</a></li>
<li class="toctree-l3"><a class="reference internal" href="draugr.torch_utilities.writers.html">draugr.torch_utilities.writers</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="draugr.tqdm_utilities.html">draugr.tqdm_utilities</a></li>
<li class="toctree-l2"><a class="reference internal" href="draugr.visualisation.html">draugr.visualisation</a></li>
<li class="toctree-l2"><a class="reference internal" href="draugr.writers.html">draugr.writers</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../getting_started.html">Getting Started</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
  <li><a href="draugr.html">draugr</a><ul>
  <li><a href="draugr.torch_utilities.html">draugr.torch_utilities</a><ul>
  <li><a href="draugr.torch_utilities.optimisation.html">draugr.torch_utilities.optimisation</a><ul>
  <li><a href="draugr.torch_utilities.optimisation.debugging.html">draugr.torch_utilities.optimisation.debugging</a><ul>
  <li><a href="draugr.torch_utilities.optimisation.debugging.gradients.html">draugr.torch_utilities.optimisation.debugging.gradients</a><ul>
  <li><a href="draugr.torch_utilities.optimisation.debugging.gradients.guided.html">draugr.torch_utilities.optimisation.debugging.gradients.guided</a><ul>
      <li>Previous: <a href="draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLUModel.html" title="previous chapter">draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLUModel</a></li>
      <li>Next: <a href="draugr.torch_utilities.optimisation.debugging.layer_fetching.html" title="next chapter">draugr.torch_utilities.optimisation.debugging.layer_fetching</a></li>
  </ul></li>
  </ul></li>
  </ul></li>
  </ul></li>
  </ul></li>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 5.0.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../_sources/generated/draugr.torch_utilities.optimisation.debugging.gradients.guided.GuidedBackPropReLU.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>